import matplotlib.pyplot as plt
import numpy as np
import os
import argparse
import nimfa
import pandas as pd
# Import csv file
data = pd.read_csv('fake_data.csv')
data = data.drop(data.columns[0], axis=1)
n_components = 2
# Change data to a numpy
V = data.to_numpy()
V = (V - V.min(0)).T
bdnmf = nimfa.Bd(V, seed="random_c", rank=n_components, max_iter=12, alpha=np.zeros((V.shape[0], n_components)),
beta=np.zeros((n_components, V.shape[1])), theta=.0, k=.0, sigma=1., skip=100, stride=1,
n_w=np.zeros((n_components, 1)), n_h=np.zeros((n_components, 1)), n_run = 1, n_sigma=False)
bdnmf_fit = bdnmf()
import matplotlib.pyplot as plt
import numpy as np
import os
import argparse
import nimfa
import pandas as pd
# Import csv file
data = pd.read_csv('fake_data.csv')
data = data.drop(data.columns[0], axis=1)
n_components = 2
# Change data to a numpy
V = data.to_numpy()
V = (V - V.min(0)).T
bdnmf = nimfa.Bd(V, seed="random_c", rank=n_components, max_iter=12, alpha=np.zeros((V.shape[0], n_components)),
beta=np.zeros((n_components, V.shape[1])), theta=.0, k=.0, sigma=1., skip=100, stride=1,
n_w=np.zeros((n_components, 1)), n_h=np.zeros((n_components, 1)), n_run = 1, n_sigma=False)
bdnmf_fit = bdnmf()
reticulate::repl_python()
setwd("~/")
reticulate::repl_python()
# Compare retrieved components
Components <- read_csv("components.csv")
# Compare retrieved components
Components <- read.csv("components.csv")
Basis <- read.csv("basis.csv", headers = FALSE)
Basis <- read.csv("basis.csv", header = FALSE)
reticulate::repl_python()
py_install(scikit-learn)
library(reticulate)
py_install(scikit-learn)
py_install(sklearn)
py_install("scikit-learn")
reticulate::repl_python()
py_install("utils-NMF")
py_install("utils-nm")
pip install utils
py_install("utils_nmf")
py_install("utils_nmf", pip = TRUE)
py_install("utils-nm", pip = TRUE)
reticulate::repl_python()
utils_nmf
library(reticulate)
library(reticulate)
reticulate::repl_python()
reticulate::py_run_string("!pip install utils_nmf")
reticulate::py_run_string("pip install utils_nmf")
reticulate::py_install("utils_nmf")
system("python -m pip install utils_nmf")
system("/Users/garethyu/Library/r-miniconda-arm64/envs/r-reticulate/bin/pip install utils_nmf")
reticulate::py_install("git+https://github.com/nloyfer/ssNMF/blob/main/utils_nmf.py", method = "pip")
system("/Users/garethyu/Library/r-miniconda-arm64/envs/r-reticulate/bin/pip install git+https://github.com/nloyfer/ssNMF/blob/main/utils_nmf.py")
download.file("https://github.com/nloyfer/ssNMF/raw/main/utils_nmf.py", destfile = "utils_nmf.py")
reticulate::repl_python()
Basis <- read.csv("/Fake Data BNMF/basis.csv", header = FALSE)
Basis <- read.csv("basis.csv", header = FALSE)
Coef <- read.csv("coef.csv", header = FALSE)
Components <- read.csv("components.csv")
Basis <- read.csv("basis.csv", header = FALSE)
Coef <- read.csv("coef.csv", header = FALSE)
Coef <- t(Coef)
Components <- read.csv("components.csv")
Basis <- read.csv("basis.csv", header = FALSE)
Coef <- read.csv("coef.csv", header = FALSE)
View(Basis)
View(Coef)
data = np.load('%sfake_data_ncomp_%d_run%d.npy' % (save_dir, n_components, r_), allow_pickle = True).item()
data = np.load('%sfake_data_ncomp_%d_run%d.npy' % (save_dir, n_components, r_), allow_pickle = True).item()
reticulate::repl_python()
setwd("~/Documents/GitHub/Natural-Stories-ICA/ICA")
set.seed(10)
numWords <- 1000
numPeople <- 100
component1 <- rnorm(numWords, mean = 0, sd = 1)
component2 <- rnorm(numWords, mean = 0, sd = 1)
individual <- rnorm(numPeople, mean = 0, sd = 1)
error <- matrix(rnorm(numWords*numPeople, mean = 0, sd = 0.5), nrow = numWords, ncol = numPeople)
beta0 <- 1
betaInd <- 1
betaC1 <- 2
betaC2 <- 3
covariance_matrix <- matrix(c(1, 0, 0, 1), nrow = 2, ncol = 2)
multipliers <- rmvnorm(numPeople, sigma = covariance_matrix, mean = c(betaC1, betaC2))
library(mvtnorm)
setwd("~/Documents/GitHub/Natural-Stories-ICA/ICA")
set.seed(10)
numWords <- 1000
numPeople <- 100
component1 <- rnorm(numWords, mean = 0, sd = 1)
component2 <- rnorm(numWords, mean = 0, sd = 1)
individual <- rnorm(numPeople, mean = 0, sd = 1)
error <- matrix(rnorm(numWords*numPeople, mean = 0, sd = 0.5), nrow = numWords, ncol = numPeople)
beta0 <- 1
betaInd <- 1
betaC1 <- 2
betaC2 <- 3
covariance_matrix <- matrix(c(1, 0, 0, 1), nrow = 2, ncol = 2)
multipliers <- rmvnorm(numPeople, sigma = covariance_matrix, mean = c(betaC1, betaC2))
fake_data <- matrix(0, nrow = numWords, ncol = numPeople)
for (i in seq_len(numPeople)) {
for (j in seq_len(numWords)) {
fake_data[j, i] <- beta0 + multipliers[i, 1]*component1[j] + multipliers[i, 2]*component2[j] + betaInd*individual[i]
}
}
fake_data <- fake_data + error
write.csv(fake_data, "~/Documents/GitHub/Natural-Stories-ICA/ICA/fake_data.csv")
write.csv(multipliers, "~/Documents/GitHub/Natural-Stories-ICA/ICA/multipliers.csv")
write.csv(cbind(component1, component2), "~/Documents/GitHub/Natural-Stories-ICA/ICA/components.csv")
library(reticulate)
reticulate::repl_python()
py_run_string("!pip install git+https://github.com/nloyfer/ssNMF/blob/main/utils_nmf.py")
py_run_string("pip install git+https://github.com/nloyfer/ssNMF/blob/main/utils_nmf.py")
system("pip install git+https://github.com/nloyfer/ssNMF.git")
reticulate::repl_python()
reticulate::repl_python()
library(dplyr)
library(cluster)
library(FNN)  # For finding nearest neighbors
library(Matrix)
# Define initial variables
n_components <- 2
save_dir <- '~/Documents/GitHub/Natural-Stories-ICA/ICA/Fake Data BNMF'
nruns <- 50
# Initialize an empty list to store responses
resp <- list()
for (r_ in 0:(nruns-1)) {
# Load data from CSV, assuming each CSV file has bases_ matrix with samples as rows
file_name <- paste0("~/Documents/GitHub/Natural-Stories-ICA/ICA/Fake Data BNMF/fake_data_ncomp_", n_components, "_run", r_, ".csv")
H <- read.csv(file_name, header = FALSE)
resp[[r_+1]] <- t(H)
}
# Combine all responses into a single matrix
resp <- do.call(rbind, resp)
combined_spectra <- as.data.frame(resp)
# Save combined spectra as CSV
write.csv(combined_spectra, sprintf('%s/spectra.csv', save_dir), row.names = TRUE)
# Load the merged spectra
merged_spectra <- as.data.frame(read.csv(sprintf('%s/spectra.csv', save_dir), row.names = 1))
density_threshold <- 0.5
k <- n_components
local_neighborhood_size <- 0.30
n_neighbors <- as.integer(local_neighborhood_size * nrow(merged_spectra) / k)
# Rescale topics to unit length (L2 normalization)
l2_spectra <- t(apply(merged_spectra, 1, function(x) x / sqrt(sum(x^2))))
# Calculate Euclidean distance matrix
topics_dist <- as.matrix(dist(l2_spectra, method = "euclidean"))
# Find partitioning order based on nearest neighbors
nearest_neighbors <- apply(topics_dist, 1, function(row) order(row)[2:(n_neighbors + 1)])
# Calculate local density based on nearest neighbors
distance_to_nearest_neighbors <- sapply(1:nrow(topics_dist), function(i) mean(topics_dist[i, nearest_neighbors[, i]]))
local_density <- data.frame(local_density = distance_to_nearest_neighbors)
rownames(local_density) <- rownames(l2_spectra)
# Apply density filter
density_filter <- local_density$local_density < density_threshold
l2_spectra_filtered <- l2_spectra[density_filter, ]
# K-means clustering
kmeans_model <- kmeans(l2_spectra_filtered, centers = k, nstart = 10, iter.max = 100)
kmeans_cluster_labels <- kmeans_model$cluster
# Calculate median usage for each cluster
median_spectra <- aggregate(as.data.frame(l2_spectra_filtered), by = list(cluster = kmeans_cluster_labels), median)
median_spectra <- median_spectra[, -1]  # Remove cluster label column
# Normalize median spectra to probability distributions
median_spectra <- t(apply(median_spectra, 1, function(row) row / sum(row)))
# Save the final response profile matrix as CSV
data_transformed <- t(median_spectra)
write.csv(data_transformed, sprintf('%s/_ncomp_%d_nruns%d.csv', save_dir, n_components, nruns), row.names = FALSE)
setwd("~/Documents/GitHub/Natural-Stories-ICA/ICA")
nruns = 50
n_components = 2
n_samples = 1000
component_list <- list()  # List to store components from each run
consensus_matrix <- matrix(0, nrow = n_components, ncol = n_samples)
component_count <- rep(0, n_components)
similarity_threshold = 0.4
for (run in 0:(nruns-1)) {
file_name <- paste0("~/Documents/GitHub/Natural-Stories-ICA/ICA/Fake Data BNMF/fake_data_ncomp_", n_components, "_run", run, ".csv")
H <- read.csv(file_name, header = FALSE)
H = t(H)
if (run == 0) {
consensus_matrix <- H  # Use the first run's components as the initial consensus
} else {
# For each component, compare with the cumulative consensus
for (comp in 1:n_components) {
# Calculate correlation with each consensus component
max_corr <- 0
best_match <- 0
for (consensus_comp in 1:n_components) {
# Check if the component has zero variance before calculating correlation
if (sd(consensus_matrix[consensus_comp, ]) > 0 && sd(H[comp, ]) > 0) {
corr <- cor(consensus_matrix[consensus_comp, ], H[comp, ])
if (!is.na(corr) && abs(corr) > max_corr) {
max_corr <- abs(corr)
best_match <- consensus_comp
}
}
}
# Update consensus if correlation is above threshold
if (max_corr >= similarity_threshold) {
consensus_matrix[best_match, ] <- (consensus_matrix[best_match, ] * component_count[best_match] + H[comp, ]) / (component_count[best_match] + 1)
component_count[best_match] <- component_count[best_match] + 1
}
}
}
}
result <- consensus_matrix
# Compare consensus components
consensus <- read.csv("~/Documents/GitHub/Natural-Stories-ICA/ICA/Fake Data BNMF/fake_data_consensus_ncomp_2_nruns50.csv", header = FALSE)
Basis <- read.csv("basis3.csv", header = FALSE)
nn_component1 <- component1 - min(component1)
nn_component2 <- component2 - min(component2)
time_points = 50
plot(scale(nn_component1[1:time_points]), type = 'l', col = 'blue', ylim = c(-3,3))
lines(scale(Basis[1:time_points, 1]), type = 'l', col = 'pink')
lines(scale(data_transformed[1:time_points, 1]), type = 'l', col = 'red')
lines(scale(result[1, 1:time_points]), type = 'l', col = 'orange')
lines(scale(consensus[1:time_points, 1]), type = 'l', col = 'darkgreen')
reticulate::repl_python()
plot(scale(nn_component2[1:time_points]), type = 'l', col = 'blue', ylim = c(-3, 3))
lines(scale(data_transformed[1:time_points, 2]), type = 'l', col = 'red')
lines(scale(Basis[1:time_points, 2]), type = 'l', col = 'pink')
lines(scale(result[2, 1:time_points]), type = 'l', col = 'orange')
lines(scale(consensus[1:time_points, 2]), type = 'l', col = 'darkgreen')
Basis <- read.csv("basis2.csv", header = FALSE)
lines(scale(Basis[1:time_points, 1]), type = 'l', col = 'pink')
plot(scale(nn_component1[1:time_points]), type = 'l', col = 'blue', ylim = c(-3,3))
lines(scale(Basis[1:time_points, 1]), type = 'l', col = 'pink')
plot(scale(nn_component2[1:time_points]), type = 'l', col = 'blue', ylim = c(-3, 3))
lines(scale(Basis[1:time_points, 2]), type = 'l', col = 'pink')
plot(scale(nn_component1[1:time_points]), type = 'l', col = 'blue', ylim = c(-3,3))
lines(scale(Basis[1:time_points, 1]), type = 'l', col = 'pink')
plot(scale(nn_component2[1:time_points]), type = 'l', col = 'blue', ylim = c(-3, 3))
lines(scale(result[2, 1:time_points]), type = 'l', col = 'orange')
lines(scale(Basis[1:time_points, 2]), type = 'l', col = 'pink')
