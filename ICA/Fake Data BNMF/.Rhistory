story_removed_cols <- select(story_raw_data, zone, WorkerId, RT, word)
story_data <- pivot_wider(story_removed_cols, names_from = WorkerId, values_from = RT)
story_data <- story_data[,-1]
story_name <- paste0("story_", story, ".csv")
write.csv(story_data, story_name)
story_data
}
for (i in 1:10) {
story_name <- paste0("s", i, "data")
assign(story_name, as.data.frame(isolate_story(processed_RTs, i)))
}
# Clean for Word Length
remove_word_length <- function(story) {
new_story <- story
for (i in 2:ncol(story)) {
subject_RT <- na.omit(story[,i])
word_lengths <- nchar(story$word[!is.na(story[,i])])
test <- lm(subject_RT ~ word_lengths)
new_story[!is.na(story[,i]),i] <- test$resid
}
new_story
}
for (i in 1:10) {
story_name <- paste0("s", i, "data")
assign(story_name, remove_word_length(get(story_name)))
}
# Remove Negativity
for (i in 1:10) {
story_name <- paste0("s", i, "data")
assign(story_name, get(story_name)[,-1] - min(get(story_name)[,-1], na.rm = TRUE))
}
# Check Missing Data Percentage
## List of matrices
stories <- list(s1data, s2data, s3data, s4data, s5data, s6data, s7data, s8data, s9data, s10data)
## Calculate the percentage of missing data for each participant
missing_percentages <- sapply(stories, function(data) {
colMeans(is.na(data)) * 100
})
## Combine Missing Percentages Across Stories
combined_missing_all <- do.call(rbind, lapply(seq_along(missing_percentages), function(i) {
data.frame(
ID = names(missing_percentages[[i]]),
missing_percent = unname(missing_percentages[[i]])
)
}))
combined_missing <- combined_missing_all %>%
group_by(ID) %>%
summarise(mean_missing = mean(missing_percent), .groups = "drop")
## Identify participants to remove (more than 10% missing data)
remove_participants <- combined_missing[which(combined_missing[2] > 10), 1]
## Remove participants from all matrices
filtered_stories <- lapply(stories, function(data) {
data[, !(colnames(data) %in% remove_participants$ID)]
})
## Unpack filtered matrices back to original variables
stories <- filtered_stories
s1data <- filtered_stories[[1]]
s2data <- filtered_stories[[2]]
s3data <- filtered_stories[[3]]
s4data <- filtered_stories[[4]]
s5data <- filtered_stories[[5]]
s6data <- filtered_stories[[6]]
s7data <- filtered_stories[[7]]
s8data <- filtered_stories[[8]]
s9data <- filtered_stories[[9]]
s10data <- filtered_stories[[10]]
# Find How Many Read Each 5
# Filling in Missing Data
remove_words <- lapply(stories, function(df) df[, -1])
zscore_data <- lapply(remove_words, function(df) apply(df, 2, scale))
impute_missing <- function(data) {
for (i in seq_len(nrow(data))) {
data[i, is.na(data[i,])] <- mean(data[i, ], na.rm = TRUE)
}
return(data)
}
zscore_data <- lapply(zscore_data, impute_missing)
anyNA(zscore_data)
scale_attributes <- lapply(remove_words, function(df) {
list(
mean = sapply(df, mean, na.rm = TRUE),
sd = sapply(df, sd, na.rm = TRUE)
)
})
# Find How Many Read Each 5
participants <- list()
colnames(stories)
# Find How Many Read Each 5
participants <- list()
colnames(stories[[1]])
# Find How Many Read Each 5
participants <- list()
colnames(stories[[1]])
lapply(stories, colnames)
# Find How Many Read Each 5
participants <- lapply(stories, colnames)
View(participants)
View(remove_words)
View(combined_missing)
library(tidyverse)
rm(list=ls())
setwd("~/Documents/GitHub/Natural-Stories-ICA/ICA")
processed_RTs <- read_tsv("processed_RTs.tsv")
# Isolate Data
isolate_story <- function(data, story) {
story_raw_data <- data[data$item == story,]
story_removed_cols <- select(story_raw_data, zone, WorkerId, RT, word)
story_data <- pivot_wider(story_removed_cols, names_from = WorkerId, values_from = RT)
story_data <- story_data[,-1]
story_name <- paste0("story_", story, ".csv")
write.csv(story_data, story_name)
story_data
}
for (i in 1:10) {
story_name <- paste0("s", i, "data")
assign(story_name, as.data.frame(isolate_story(processed_RTs, i)))
}
# Clean for Word Length
remove_word_length <- function(story) {
new_story <- story
for (i in 2:ncol(story)) {
subject_RT <- na.omit(story[,i])
word_lengths <- nchar(story$word[!is.na(story[,i])])
test <- lm(subject_RT ~ word_lengths)
new_story[!is.na(story[,i]),i] <- test$resid
}
new_story
}
for (i in 1:10) {
story_name <- paste0("s", i, "data")
assign(story_name, remove_word_length(get(story_name)))
}
# Remove Negativity
for (i in 1:10) {
story_name <- paste0("s", i, "data")
assign(story_name, get(story_name)[,-1] - min(get(story_name)[,-1], na.rm = TRUE))
}
# Check Missing Data Percentage
## List of matrices
stories <- list(s1data, s2data, s3data, s4data, s5data, s6data, s7data, s8data, s9data, s10data)
## Calculate the percentage of missing data for each participant
missing_percentages <- sapply(stories, function(data) {
colMeans(is.na(data)) * 100
})
## Combine Missing Percentages Across Stories
combined_missing_all <- do.call(rbind, lapply(seq_along(missing_percentages), function(i) {
data.frame(
ID = names(missing_percentages[[i]]),
missing_percent = unname(missing_percentages[[i]])
)
}))
combined_missing <- combined_missing_all %>%
group_by(ID) %>%
summarise(mean_missing = mean(missing_percent), .groups = "drop")
## Identify participants to remove (more than 10% missing data)
remove_participants <- combined_missing[which(combined_missing[2] > 10), 1]
View(combined_missing)
View(s1data)
View(combined_missing_all)
View(combined_missing)
## Remove participants from all matrices
filtered_stories <- lapply(stories, function(data) {
data[, !(colnames(data) %in% remove_participants$ID)]
})
## Unpack filtered matrices back to original variables
stories <- filtered_stories
s1data <- filtered_stories[[1]]
s2data <- filtered_stories[[2]]
s3data <- filtered_stories[[3]]
s4data <- filtered_stories[[4]]
s5data <- filtered_stories[[5]]
s6data <- filtered_stories[[6]]
s7data <- filtered_stories[[7]]
s8data <- filtered_stories[[8]]
s9data <- filtered_stories[[9]]
s10data <- filtered_stories[[10]]
View(missing_percentages)
# Find How Many Read Each 5
participants <- lapply(stories, colnames)
unique(unlist(participants))
library(tidyverse)
rm(list=ls())
setwd("~/Documents/GitHub/Natural-Stories-ICA/ICA")
processed_RTs <- read_tsv("processed_RTs.tsv")
# Isolate Data
isolate_story <- function(data, story) {
story_raw_data <- data[data$item == story,]
story_removed_cols <- select(story_raw_data, zone, WorkerId, RT, word)
story_data <- pivot_wider(story_removed_cols, names_from = WorkerId, values_from = RT)
story_data <- story_data[,-1]
story_name <- paste0("story_", story, ".csv")
write.csv(story_data, story_name)
story_data
}
for (i in 1:10) {
story_name <- paste0("s", i, "data")
assign(story_name, as.data.frame(isolate_story(processed_RTs, i)))
}
# Clean for Word Length
remove_word_length <- function(story) {
new_story <- story
for (i in 2:ncol(story)) {
subject_RT <- na.omit(story[,i])
word_lengths <- nchar(story$word[!is.na(story[,i])])
test <- lm(subject_RT ~ word_lengths)
new_story[!is.na(story[,i]),i] <- test$resid
}
new_story
}
for (i in 1:10) {
story_name <- paste0("s", i, "data")
assign(story_name, remove_word_length(get(story_name)))
}
# Remove Negativity
for (i in 1:10) {
story_name <- paste0("s", i, "data")
assign(story_name, get(story_name)[,-1] - min(get(story_name)[,-1], na.rm = TRUE))
}
# Find How Many Read Each 5
participants <- lapply(stories, colnames)
# Check Missing Data Percentage
## List of matrices
stories <- list(s1data, s2data, s3data, s4data, s5data, s6data, s7data, s8data, s9data, s10data)
# Find How Many Read Each 5
participants <- lapply(stories, colnames)
unique(unlist(participants))
# Find How Many Read Each 5
participants <- lapply(stories, colnames)
length(unique(unlist(participants)))
library(tidyverse)
rm(list=ls())
setwd("~/Documents/GitHub/Natural-Stories-ICA/ICA")
processed_RTs <- read_tsv("processed_RTs.tsv")
# Isolate Data
isolate_story <- function(data, story) {
story_raw_data <- data[data$item == story,]
story_removed_cols <- select(story_raw_data, zone, WorkerId, RT, word)
story_data <- pivot_wider(story_removed_cols, names_from = WorkerId, values_from = RT)
story_data <- story_data[,-1]
story_name <- paste0("story_", story, ".csv")
write.csv(story_data, story_name)
story_data
}
for (i in 1:10) {
story_name <- paste0("s", i, "data")
assign(story_name, as.data.frame(isolate_story(processed_RTs, i)))
}
# Clean for Word Length
remove_word_length <- function(story) {
new_story <- story
for (i in 2:ncol(story)) {
subject_RT <- na.omit(story[,i])
word_lengths <- nchar(story$word[!is.na(story[,i])])
test <- lm(subject_RT ~ word_lengths)
new_story[!is.na(story[,i]),i] <- test$resid
}
new_story
}
for (i in 1:10) {
story_name <- paste0("s", i, "data")
assign(story_name, remove_word_length(get(story_name)))
}
# Remove Negativity
for (i in 1:10) {
story_name <- paste0("s", i, "data")
assign(story_name, get(story_name)[,-1] - min(get(story_name)[,-1], na.rm = TRUE))
}
# Check Missing Data Percentage
## List of matrices
stories <- list(s1data, s2data, s3data, s4data, s5data, s6data, s7data, s8data, s9data, s10data)
## Calculate the percentage of missing data for each participant
missing_percentages <- sapply(stories, function(data) {
colMeans(is.na(data)) * 100
})
## Combine Missing Percentages Across Stories
combined_missing_all <- do.call(rbind, lapply(seq_along(missing_percentages), function(i) {
data.frame(
ID = names(missing_percentages[[i]]),
missing_percent = unname(missing_percentages[[i]])
)
}))
combined_missing <- combined_missing_all %>%
group_by(ID) %>%
summarise(mean_missing = mean(missing_percent), .groups = "drop")
## Identify participants to remove (more than 10% missing data)
remove_participants <- combined_missing[which(combined_missing[2] > 10), 1]
## Remove participants from all matrices
filtered_stories <- lapply(stories, function(data) {
data[, !(colnames(data) %in% remove_participants$ID)]
})
## Unpack filtered matrices back to original variables
stories <- filtered_stories
s1data <- filtered_stories[[1]]
s2data <- filtered_stories[[2]]
s3data <- filtered_stories[[3]]
s4data <- filtered_stories[[4]]
s5data <- filtered_stories[[5]]
s6data <- filtered_stories[[6]]
s7data <- filtered_stories[[7]]
s8data <- filtered_stories[[8]]
s9data <- filtered_stories[[9]]
s10data <- filtered_stories[[10]]
# Find How Many Read Each 5
participants <- lapply(stories, colnames)
length(unique(unlist(participants)))
# Find How Many Read Each 5
story_participants <- lapply(stories, colnames)
participant_IDs <- unique(unlist(participants)))
# Find How Many Read Each 5
story_participants <- lapply(stories, colnames)
participant_IDs <- unique(unlist(participants))
which()
# Find How Many Read Each 5
story_participants <- lapply(stories, colnames)
participant_IDs <- unique(unlist(participants))
participant_IDs %in% story_participants
# Find How Many Read Each 5
story_participants <- lapply(stories, colnames)
participant_IDs <- unique(unlist(participants))
participant_IDs %in% story_participants[[1]]
# Find which stories each participant read
story_participants <- lapply(participant_IDs, function(participant) {
which(sapply(stories, function(story) participant %in% story))
})
## Find which stories each participant read
participant_stories <- lapply(participant_IDs, function(participant) {
which(sapply(story_participants, function(story) participant %in% story))
})
# Assign participant IDs as names for easy interpretation
names(participant_stories) <- participant_IDs
# View the result
participant_stories
s1data$A38NIARZOSEVUG
s2data$A38NIARZOSEVUG
s3data$A38NIARZOSEVUG
s4data$A38NIARZOSEVUG
s5data$A38NIARZOSEVUG
s6data$A38NIARZOSEVUG
s7data$A38NIARZOSEVUG
s8data$A38NIARZOSEVUG
s9data$A38NIARZOSEVUG
s10data$A38NIARZOSEVUG
# Assign participant IDs as names for easy interpretation
names(participant_stories) <- participant_IDs
# View the result
participant_stories
View(combined_missing)
my_matrix <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9), nrow = 3, ncol = 3)
scale(my_matrix)
library(mvtnorm)
library(reticulate)
setwd("~/Documents/GitHub/Natural-Stories-ICA/ICA/Fake Data BNMF/")
set.seed(10)
numWords <- 1000
numPeople <- 100
component1 <- rnorm(numWords, mean = 0, sd = 1)
component2 <- rnorm(numWords, mean = 0, sd = 1)
component3 <- rnorm(numWords, mean = 0, sd = 1)
component4 <- rnorm(numWords, mean = 0, sd = 1)
individual <- rnorm(numPeople, mean = 0, sd = 1)
error <- matrix(rnorm(numWords*numPeople, mean = 0, sd = 0.8), nrow = numWords, ncol = numPeople)
beta0 <- 1
betaInd <- 1
betaC1 <- 1
betaC2 <- 2
betaC3 <- 3
betaC4 <- 4
covariance_matrix <- diag(4)
multipliers <- rmvnorm(numPeople, sigma = covariance_matrix, mean = c(betaC1, betaC2, betaC3, betaC4))
fake_data <- matrix(0, nrow = numWords, ncol = numPeople)
for (i in seq_len(numPeople)) {
for (j in seq_len(numWords)) {
fake_data[j, i] <- beta0 + multipliers[i, 1]*component1[j] + multipliers[i, 2]*component2[j] +
multipliers[i, 3]*component3[j] + multipliers[i, 4]*component4[j] +
betaInd*individual[i]
}
}
fake_data <- fake_data + error
write.csv(scale(fake_data), "fake_data.csv")
write.csv(multipliers, "multipliers.csv")
write.csv(cbind(component1, component2, component3, component4), "components.csv")
components <- read.csv("components.csv")[,-1]
reticulate::repl_python()
Basis <- read.csv("fake_data_ncomp_4_run1.csv", header = FALSE)
consensus <- read.csv("fake_data_consensus_ncomp_4_nruns50.csv", header = FALSE)
cor(components, consensus)
py$H
multipliers
cor(multipliers, py$H)
cor(multipliers, t(py$H))
library(mvtnorm)
library(reticulate)
setwd("~/Documents/GitHub/Natural-Stories-ICA/ICA/Fake Data BNMF/")
set.seed(10)
numWords <- 1000
numPeople <- 100
component1 <- rnorm(numWords, mean = 0, sd = 1)
component2 <- rnorm(numWords, mean = 0, sd = 1)
component3 <- rnorm(numWords, mean = 0, sd = 1)
component4 <- rnorm(numWords, mean = 0, sd = 1)
individual <- rnorm(numPeople, mean = 0, sd = 1)
error <- matrix(rnorm(numWords*numPeople, mean = 0, sd = 0.8), nrow = numWords, ncol = numPeople)
beta0 <- 1
betaInd <- 1
betaC1 <- 1
betaC2 <- 2
betaC3 <- 3
betaC4 <- 4
covariance_matrix <- diag(4)
multipliers <- rmvnorm(numPeople, sigma = covariance_matrix, mean = c(betaC1, betaC2, betaC3, betaC4))
fake_data <- matrix(0, nrow = numWords, ncol = numPeople)
for (i in seq_len(numPeople)) {
for (j in seq_len(numWords)) {
fake_data[j, i] <- beta0 + multipliers[i, 1]*component1[j] + multipliers[i, 2]*component2[j] +
multipliers[i, 3]*component3[j] + multipliers[i, 4]*component4[j] +
betaInd*individual[i]
}
}
fake_data <- fake_data + error
write.csv(fake_data, "fake_data.csv")
write.csv(multipliers, "multipliers.csv")
write.csv(cbind(component1, component2, component3, component4), "components.csv")
components <- read.csv("components.csv")[,-1]
reticulate::repl_python()
# Fake Data Creation
library(mvtnorm)
library(reticulate)
setwd("~/Documents/GitHub/Natural-Stories-ICA/ICA/Fake Data BNMF/")
set.seed(10)
numWords <- 1000
numPeople <- 100
component1 <- rnorm(numWords, mean = 0, sd = 1)
component2 <- rnorm(numWords, mean = 0, sd = 1)
component3 <- rnorm(numWords, mean = 0, sd = 1)
component4 <- rnorm(numWords, mean = 0, sd = 1)
individual <- rnorm(numPeople, mean = 0, sd = 1)
error <- matrix(rnorm(numWords*numPeople, mean = 0, sd = 0.8), nrow = numWords, ncol = numPeople)
beta0 <- 1
betaInd <- 1
betaC1 <- 1
betaC2 <- 2
betaC3 <- 3
betaC4 <- 4
component1 <- component1 - min(component1)
component2 <- component2 - min(component2)
component3 <- component3 - min(component3)
component4 <- component4 - min(component4)
covariance_matrix <- diag(4)
multipliers <- rmvnorm(numPeople, sigma = covariance_matrix, mean = c(betaC1, betaC2, betaC3, betaC4))
fake_data <- matrix(0, nrow = numWords, ncol = numPeople)
for (i in seq_len(numPeople)) {
for (j in seq_len(numWords)) {
fake_data[j, i] <- beta0 + multipliers[i, 1]*component1[j] + multipliers[i, 2]*component2[j] +
multipliers[i, 3]*component3[j] + multipliers[i, 4]*component4[j] +
betaInd*individual[i]
}
}
fake_data <- fake_data + error
write.csv(fake_data, "fake_data.csv")
write.csv(multipliers, "multipliers.csv")
write.csv(cbind(component1, component2, component3, component4), "components.csv")
components <- read.csv("components.csv")[,-1]
reticulate::repl_python()
plot(component1)
reticulate::repl_python()
consensus <- read.csv("fake_data_consensus_ncomp_4_nruns50.csv", header = FALSE)
cor(components, consensus)
cor(multipliers, t(py$H))
reticulate::repl_python()
consensus <- read.csv("fake_data_consensus_ncomp_4_nruns50.csv", header = FALSE)
cor(components, consensus)
reticulate::repl_python()
consensus <- read.csv("fake_data_consensus_ncomp_4_nruns50.csv", header = FALSE)
cor(components, consensus)
library(mvtnorm)
library(reticulate)
setwd("~/Documents/GitHub/Natural-Stories-ICA/ICA/Fake Data BNMF/")
set.seed(10)
numWords <- 1000
numPeople <- 100
component1 <- rnorm(numWords, mean = 0, sd = 1)
component2 <- rnorm(numWords, mean = 0, sd = 1)
component3 <- rnorm(numWords, mean = 0, sd = 1)
component4 <- rnorm(numWords, mean = 0, sd = 1)
individual <- rnorm(numPeople, mean = 0, sd = 1)
error <- matrix(rnorm(numWords*numPeople, mean = 0, sd = 0.8), nrow = numWords, ncol = numPeople)
beta0 <- 1
betaInd <- 1
betaC1 <- 1
betaC2 <- 2
betaC3 <- 3
betaC4 <- 4
component1 <- abs(component1)
component2 <- abs(component2)
component3 <- abs(component3)
component4 <- abs(component4)
covariance_matrix <- diag(4)
multipliers <- rmvnorm(numPeople, sigma = covariance_matrix, mean = c(betaC1, betaC2, betaC3, betaC4))
fake_data <- matrix(0, nrow = numWords, ncol = numPeople)
for (i in seq_len(numPeople)) {
for (j in seq_len(numWords)) {
fake_data[j, i] <- beta0 + multipliers[i, 1]*component1[j] + multipliers[i, 2]*component2[j] +
multipliers[i, 3]*component3[j] + multipliers[i, 4]*component4[j] +
betaInd*individual[i]
}
}
fake_data <- fake_data + error
write.csv(fake_data, "fake_data.csv")
write.csv(multipliers, "multipliers.csv")
write.csv(cbind(component1, component2, component3, component4), "components.csv")
components <- read.csv("components.csv")[,-1]
reticulate::repl_python()
